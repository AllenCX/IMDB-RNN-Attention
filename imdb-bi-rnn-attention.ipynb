{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from config import Config\n",
    "from utils import *\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class imdb_classifier(object):\n",
    "    def __init__(self, config, session, x_train, y_train, x_test, y_test, train_length, test_lentgh):\n",
    "        self.config = config\n",
    "        self.embedding_size = config.embedding_size\n",
    "        self.batch_size = config.batch_size\n",
    "        self.encoder_hidden_size = config.encoder_hidden_size\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lr = config.lr\n",
    "        self.sess = session\n",
    "        self.epoch_num = config.epoch_num\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "        self.train_length = train_length\n",
    "        self.test_length = test_length\n",
    "        self.max_length = config.max_length\n",
    "        self.max_grad_norm = config.max_grad_norm\n",
    "        self.save_per_epoch = config.save_per_epoch\n",
    "        self.ckpt_path = config.ckpt_path\n",
    "        self.test_lentgh = test_length\n",
    "        self.keep_prob = config.keep_prob\n",
    "        self.atn_hidden_size = config.atn_hidden_size\n",
    "        \n",
    "    def build(self):\n",
    "        self.global_step = tf.Variable(0, name=\"global_step\")\n",
    "        self.batch_maxlen = tf.placeholder(dtype=tf.int32, name=\"batch_maxlen\")\n",
    "        self.output_keep_prob = tf.placeholder(dtype=tf.float32, name=\"output_keep_prob\")\n",
    "        self.encoder_input = tf.placeholder(shape=(None, None), dtype=tf.int32, name=\"encoder_input\")\n",
    "        self.encoder_input_length = tf.placeholder(shape=(None,), dtype=tf.int32, name=\"encoder_input_length\")\n",
    "        self.labels = tf.placeholder(shape=(None,), dtype=tf.int32, name=\"label\")\n",
    "        self.embedding = tf.Variable(tf.random_uniform([self.vocab_size, self.embedding_size], -5.0, 5.0), \n",
    "                                     dtype=tf.float32,\n",
    "                                     trainable=False,\n",
    "                                     name=\"embedding\")\n",
    "        self.encoder_input_embedded = tf.nn.embedding_lookup(self.embedding, \n",
    "                                                             self.encoder_input)\n",
    "        self.encoder_input_embeded = tf.nn.dropout(self.encoder_input_embedded, keep_prob=self.output_keep_prob)\n",
    "        \n",
    "        # bidirectional GRU with dropout\n",
    "        encoder_fw = tf.contrib.rnn.GRUCell(self.encoder_hidden_size)\n",
    "        encoder_bw = tf.contrib.rnn.GRUCell(self.encoder_hidden_size)\n",
    "        self.encoder_fw = tf.contrib.rnn.DropoutWrapper(encoder_fw, output_keep_prob=self.output_keep_prob)\n",
    "        self.encoder_bw = tf.contrib.rnn.DropoutWrapper(encoder_bw, output_keep_prob=self.output_keep_prob)\n",
    "        \n",
    "        # Since time_major == False, output shape should be [batch_size, max_time, ...]\n",
    "        # run the GRU\n",
    "        with tf.variable_scope(\"bi-GRU\") as scope:\n",
    "            ((self.encoder_fw_output, self.encoder_bw_output), \n",
    "             (self.encoder_fw_state, self.encoder_bw_state)) = (\n",
    "                tf.nn.bidirectional_dynamic_rnn(cell_fw=self.encoder_fw, \n",
    "                                                cell_bw=self.encoder_bw, \n",
    "                                                inputs=self.encoder_input_embedded,\n",
    "                                                sequence_length=self.encoder_input_length,\n",
    "                                                dtype=tf.float32)\n",
    "            )\n",
    "\n",
    "            self.encoder_output = tf.concat((self.encoder_fw_output, self.encoder_bw_output), 2) \n",
    "            #[batch_size, max_time, 2 * encoder_hidden_size]\n",
    "            self.encoder_state = tf.concat((self.encoder_fw_state, self.encoder_bw_state), 1)\n",
    "        \n",
    "        # Attention layer\n",
    "        with tf.variable_scope(\"attention\") as scope:\n",
    "            self._atn_in = tf.expand_dims(self.encoder_output, axis=2) # [batch_size, max_time, 1, 2 * encoder_hidden_size]\n",
    "            self.atn_w = tf.Variable(\n",
    "                tf.truncated_normal(shape=[1, 1, 2 * self.encoder_hidden_size, self.atn_hidden_size], stddev=0.1),\n",
    "                name=\"atn_w\")\n",
    "            self.atn_b = tf.Variable(tf.zeros(shape=[self.atn_hidden_size]))\n",
    "            self.atn_v = tf.Variable(\n",
    "                tf.truncated_normal(shape=[1, 1, self.atn_hidden_size, 1], stddev=0.1),\n",
    "                name=\"atn_b\")\n",
    "            self.atn_activations = tf.nn.tanh(\n",
    "                tf.nn.conv2d(self._atn_in, self.atn_w, strides=[1,1,1,1], padding='SAME') + self.atn_b)\n",
    "            self.atn_scores = tf.nn.conv2d(self.atn_activations, self.atn_v, strides=[1,1,1,1], padding='SAME')\n",
    "            atn_probs = tf.nn.softmax(tf.squeeze(self.atn_scores, [2, 3]))\n",
    "            _atn_out = tf.matmul(tf.expand_dims(atn_probs, 1), self.encoder_output)\n",
    "            self.atn_out = tf.squeeze(_atn_out, [1], name=\"atn_out\")\n",
    "            \n",
    "        # Output layer\n",
    "        with tf.variable_scope(\"output\") as scope:\n",
    "            self.output_w = tf.Variable(\n",
    "                tf.truncated_normal(shape=(self.encoder_hidden_size*2, 2), stddev=0.1), name=\"output_w\") \n",
    "            self.output_b = tf.Variable(tf.zeros(2), name=\"output_b\")\n",
    "\n",
    "            self.logits = tf.matmul(self.atn_out, self.output_w) + self.output_b\n",
    "            self.prediction = tf.cast(tf.argmax(self.logits, 1), tf.int32)\n",
    "\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(tf.equal(self.prediction, self.labels), tf.float32))\n",
    "\n",
    "            self.loss = tf.reduce_mean(\n",
    "                tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.labels)\n",
    "            )\n",
    "        \n",
    "        '''self.tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, self.tvars),\n",
    "                                          self.max_grad_norm)\n",
    "        \n",
    "        self.opt = tf.train.AdamOptimizer(self.lr)\n",
    "        self.train_op = self.opt.apply_gradients(zip(grads, self.tvars), global_step=self.global_step)'''\n",
    "        self.train_op = tf.train.AdamOptimizer(learning_rate=self.lr).minimize(self.loss, global_step=self.global_step)\n",
    "        #self.rmsp_train_op = tf.train.RMSPropOptimizer(learning_rate=self.lr).minimize(self.loss, global_step=self.global_step)\n",
    "        \n",
    "        self.saver = tf.train.Saver()\n",
    "    def train(self, mode=None, restore=False):\n",
    "        if mode != \"continue\":   \n",
    "            print(\"Building the model...\")\n",
    "            self.build()\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "        else:\n",
    "            if restore:\n",
    "                self.saver.restore(sess=self.sess, save_path=self.ckpt_path)\n",
    "        \n",
    "        print(\"Starting training...\")\n",
    "        print(\"%d steps per epoch.\" % (len(self.x_train) // self.batch_size))\n",
    "        for epoch in range(self.epoch_num):\n",
    "            loss_in_epoch = []\n",
    "            acc_in_epoch = []\n",
    "            for x_batch, y_batch, input_length in self.minibatches(\n",
    "                self.x_train, self.y_train, self.train_length, batch_size=self.batch_size, shuffle=True):\n",
    "                # pad inputs\n",
    "                x_batch, batch_maxlen = self.padding_sequence(x_batch, self.max_length)\n",
    "                feed_dict = {\n",
    "                    self.encoder_input: x_batch,\n",
    "                    self.encoder_input_length: input_length,\n",
    "                    self.labels: y_batch,\n",
    "                    self.output_keep_prob: self.keep_prob,\n",
    "                    self.batch_maxlen: batch_maxlen\n",
    "                }\n",
    "                _, loss, step, acc, pred = sess.run(\n",
    "                    [self.train_op, self.loss, self.global_step, self.accuracy, self.prediction], feed_dict=feed_dict)\n",
    "                loss_in_epoch.append(loss)\n",
    "                acc_in_epoch.append(acc)\n",
    "                sys.stdout.write(\"Epoch %d, Step: %d, Loss: %.4f, Acc: %.4f\\r\" % (epoch, step, loss, acc))\n",
    "                sys.stdout.flush()\n",
    "                \n",
    "            sys.stdout.write(\"Epoch %d, Step: %d, Loss: %.4f, Acc: %.4f\\r\" % \n",
    "                             (epoch, step, np.mean(loss_in_epoch), np.mean(acc_in_epoch)))\n",
    "            sys.stdout.flush()\n",
    "            print(\"\")\n",
    "            if (epoch + 1) % self.save_per_epoch == 0:\n",
    "                self.saver.save(self.sess, \"models/bi-lstm-imdb.ckpt\")\n",
    "                self.test(sub_size=5000, restore=False)\n",
    "                \n",
    "    \n",
    "    def test(self, sub_size=None, restore=False):\n",
    "        if sub_size == None:\n",
    "            train_sub_size = len(self.y_train)\n",
    "            test_sub_size = len(self.y_test)\n",
    "        else:\n",
    "            train_sub_size = sub_size\n",
    "            test_sub_size = sub_size \n",
    "        \n",
    "        # build and restore the model\n",
    "        if restore:\n",
    "            self.build()\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "            self.saver.restore(sess=self.sess, save_path=self.ckpt_path)\n",
    "\n",
    "        acc_list = []\n",
    "        loss_list = []\n",
    "        for x_batch, y_batch, input_length in self.minibatches(\n",
    "            self.x_train[:train_sub_size], self.y_train[:train_sub_size], self.train_length[:train_sub_size], self.batch_size, False):\n",
    "            x_batch, _ = self.padding_sequence(x_batch, self.max_length)\n",
    "            feed_dict = {\n",
    "                self.encoder_input: x_batch,\n",
    "                self.encoder_input_length: input_length,\n",
    "                self.labels: y_batch,\n",
    "                self.output_keep_prob: 1.0\n",
    "            }\n",
    "            loss, acc, pred = self.sess.run([self.loss, self.accuracy, self.prediction], feed_dict=feed_dict)\n",
    "            acc_list.append(acc)\n",
    "            loss_list.append(loss)\n",
    "            '''print(pred)\n",
    "            print(y_batch)\n",
    "            print(acc, np.mean(pred == y_batch))\n",
    "            return '''\n",
    "        print(\"Test finished on training set! Loss: %.4f, Acc: %.4f\" % (np.mean(loss_list), np.mean(acc_list)))\n",
    "        \n",
    "        acc_list = []\n",
    "        loss_list = []\n",
    "        for x_batch, y_batch, input_length in self.minibatches(\n",
    "            self.x_test[:test_sub_size], self.y_test[:test_sub_size], self.test_lentgh[:test_sub_size], self.batch_size, False):\n",
    "            x_batch, _ = self.padding_sequence(x_batch, self.max_length)\n",
    "            feed_dict = {\n",
    "                self.encoder_input: x_batch,\n",
    "                self.encoder_input_length: input_length,\n",
    "                self.labels: y_batch,\n",
    "                self.output_keep_prob: 1.0\n",
    "            }\n",
    "            loss, acc = self.sess.run([self.loss, self.accuracy], feed_dict=feed_dict)\n",
    "            acc_list.append(acc)\n",
    "            loss_list.append(loss)\n",
    "        print(\"Test finished on test set! Loss: %.4f, Acc: %.4f\" % (np.mean(loss_list), np.mean(acc_list)))\n",
    "        \n",
    "    def predict(self, inputs, restore=False):\n",
    "        if restore:\n",
    "            self.build()\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "            self.saver.restore(sess=self.sess, save_path=self.ckpt_path)\n",
    "\n",
    "        inputs = self.padding_sequence(inputs)\n",
    "        inputs_length = np.array([len(seq) for seq in inputs])\n",
    "        feed_dict = {\n",
    "             self.encoder_input: inputs,\n",
    "            self.encoder_input_length: input_length,\n",
    "            self.output_keep_prob: 1.0\n",
    "        }\n",
    "        pred = self.sess.run()\n",
    "    def minibatches(self, inputs=None, targets=None, input_len=None, batch_size=None, shuffle=True):\n",
    "        assert len(inputs) == len(targets)\n",
    "        #assert len(inputs) == len(inputs_length)\n",
    "        if shuffle:\n",
    "            indices = np.arange(len(inputs))\n",
    "            np.random.shuffle(indices)\n",
    "        for start_idx in range(0, len(inputs) - batch_size + 1, batch_size):\n",
    "            if shuffle:\n",
    "                excerpt = indices[start_idx:start_idx + batch_size]\n",
    "            else:\n",
    "                excerpt = slice(start_idx, start_idx + batch_size)\n",
    "            yield inputs[excerpt], targets[excerpt], input_len[excerpt]\n",
    "    \n",
    "    def padding_sequence(self, inputs, max_length=None):\n",
    "        batch_size = len(inputs)\n",
    "        #assert self.batch_size == batch_size\n",
    "        if max_length != None:\n",
    "            if np.max([len(i) for i in inputs]) > max_length:\n",
    "                maxlen = max_length\n",
    "            else:\n",
    "                maxlen = np.max([len(i) for i in inputs])\n",
    "        else:\n",
    "            maxlen = np.max([len(i) for i in inputs])\n",
    "        \n",
    "        output = np.zeros([batch_size, maxlen], dtype=np.int32) \n",
    "        for i, seq in enumerate(inputs):     \n",
    "            output[i, :len(seq[:maxlen])] = np.array(seq[:maxlen])\n",
    "        return output, maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = Config(batch_size=32, \n",
    "                embedding_size=128,\n",
    "                encoder_hidden_size=64,\n",
    "                vocab_size=88584,\n",
    "                lr=0.0005, \n",
    "                epoch_num=50,\n",
    "                save_per_epoch=5,\n",
    "                max_length=128,\n",
    "                max_grad_norm=5,\n",
    "                keep_prob=0.2,\n",
    "                atn_hidden_size=16,\n",
    "                ckpt_path=\"models\")\n",
    "\n",
    "x_train, y_train, x_test, y_test, train_length, test_length, wid_dict, id2w = load_imdb_data()\n",
    "\n",
    "SAMPLE_SIZE = 25000\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "test = imdb_classifier(config, \n",
    "                       sess, \n",
    "                       x_train[:SAMPLE_SIZE], \n",
    "                       y_train[:SAMPLE_SIZE], \n",
    "                       x_test[:SAMPLE_SIZE], \n",
    "                       y_test[:SAMPLE_SIZE], \n",
    "                       train_length, \n",
    "                       test_length)\n",
    "test.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
